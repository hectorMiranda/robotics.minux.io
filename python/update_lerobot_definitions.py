import requests
import re
import json
import os

README_URL = "https://raw.githubusercontent.com/huggingface/lerobot/main/README.md"
OUTPUT_PATH = "baremetal/lerobot_definitions.h"

# Keywords you want to extract definitions for
KEY_TERMS = [
    "dataset", "episode", "parquet", "conda", "hf_dataset", "action", "observation", "robot", "LeRobot", "controller"
]

def extract_definitions(markdown, terms):
    definitions = {}
    lines = markdown.splitlines()
    for term in terms:
        pattern = re.compile(rf"\b{re.escape(term)}\b.*?[:\-–]\s*(.*)", re.IGNORECASE)
        for line in lines:
            match = pattern.search(line)
            if match:
                definition = match.group(1).strip()
                if len(definition) > 10:
                    definitions[term] = definition
                    break
    return definitions

def write_header(definitions, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)  # ✅ Create directory if missing
    with open(path, "w") as f:
        f.write("// Auto-generated by update_lerobot_definitions.py\n")
        f.write("#include <map>\n#include <string>\n\n")
        f.write("inline std::map<std::string, std::string> get_lerobot_definitions() {\n")
        f.write("    return {\n")
        for k, v in definitions.items():
            f.write(f'        {{"{k}", "{v}"}},\n')
        f.write("    };\n}\n")

def main():
    print("Fetching README...")
    res = requests.get(README_URL)
    res.raise_for_status()

    md = res.text
    defs = extract_definitions(md, KEY_TERMS)
    if not defs:
        print("No definitions found.")
        return

    print(f"Writing {len(defs)} definitions to header...")
    write_header(defs, OUTPUT_PATH)
    print(f"Done: {OUTPUT_PATH}")

if __name__ == "__main__":
    main()
