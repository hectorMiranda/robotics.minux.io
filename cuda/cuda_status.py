#!/usr/bin/env python3

import subprocess
import shutil
import os

def check_nvidia_smi():
    print("\nüîç Checking `nvidia-smi`...")
    if not shutil.which("nvidia-smi"):
        print("‚ùå `nvidia-smi` not found in PATH. Is the NVIDIA driver installed?")
        return False
    try:
        output = subprocess.check_output(["nvidia-smi"], stderr=subprocess.STDOUT)
        print("‚úÖ `nvidia-smi` found:\n")
        print(output.decode())
        return True
    except subprocess.CalledProcessError as e:
        print("‚ùå Error running `nvidia-smi`:\n", e.output.decode())
        return False

def check_nvcc():
    print("\nüîç Checking `nvcc` (CUDA compiler)...")
    if not shutil.which("nvcc"):
        print("‚ùå `nvcc` not found in PATH. CUDA Toolkit may not be installed.")
        return False
    try:
        output = subprocess.check_output(["nvcc", "--version"], stderr=subprocess.STDOUT)
        print("‚úÖ `nvcc` found:")
        print(output.decode())
        return True
    except subprocess.CalledProcessError as e:
        print("‚ùå Error running `nvcc`:\n", e.output.decode())
        return False

def check_cuda_toolkit():
    print("\nüîç Checking for CUDA Toolkit directory...")
    cuda_path = "/usr/local/cuda"
    if os.path.isdir(cuda_path):
        print(f"‚úÖ CUDA Toolkit directory found at {cuda_path}")
        return True
    else:
        print(f"‚ùå CUDA Toolkit directory not found at {cuda_path}")
        return False

def check_pytorch_cuda():
    print("\nüîç Checking PyTorch CUDA support...")
    try:
        import torch
        print(f"PyTorch version: {torch.__version__}")
        if torch.cuda.is_available():
            print(f"‚úÖ CUDA is available. Device: {torch.cuda.get_device_name(0)}")
            return True
        else:
            print("‚ùå CUDA not available via PyTorch.")
            return False
    except ImportError:
        print("‚ö†Ô∏è PyTorch is not installed. Skipping CUDA test via torch.")
        return None

def check_tensorflow_cuda():
    print("\nüîç Checking TensorFlow CUDA support...")
    try:
        import tensorflow as tf
        print(f"TensorFlow version: {tf.__version__}")
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"‚úÖ CUDA is available. GPUs: {[gpu.name for gpu in gpus]}")
            return True
        else:
            print("‚ùå CUDA not available via TensorFlow.")
            return False
    except ImportError:
        print("‚ö†Ô∏è TensorFlow is not installed. Skipping CUDA test via tensorflow.")
        return None

def print_summary(results):
    print("\n================ CUDA STATUS SUMMARY ================")
    for name, result in results.items():
        if result is True:
            print(f"‚úÖ {name}: OK")
        elif result is False:
            print(f"‚ùå {name}: NOT OK")
        else:
            print(f"‚ö†Ô∏è {name}: Not tested (missing package)")
    print("====================================================\n")

if __name__ == "__main__":
    results = {}
    results['nvidia-smi'] = check_nvidia_smi()
    results['nvcc'] = check_nvcc()
    results['CUDA Toolkit dir'] = check_cuda_toolkit()
    results['PyTorch CUDA'] = check_pytorch_cuda()
    results['TensorFlow CUDA'] = check_tensorflow_cuda()
    print_summary(results)
